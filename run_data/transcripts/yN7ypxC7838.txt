Welcome to Whiteboard Programming, where we simplify programming for you with easy-to-understand whiteboard videos. And today, I'll be giving you a brief explanation of all machine learning models. So let's get started. Broadly speaking, all machine learning models can be categorized as supervised or unsupervised. We'll uncover each one of them and what all types they have. Number 1. Supervised Learning. This is a series of functions that maps an input to an output based on a series of example input-output pairs. For example, if we have a dataset of two variables, one being age, which is the input, and other being the shoe size, as output, we could implement a supervised learning model to predict the shoe size of a person based on their age. Further, with supervised learning, there are two subcategories. One is regression and other is classification. In a regression model, we find a target value based on independent predictors. That means you can use this to find the relationship between a dependent variable and an independent variable. In regression models, the output is continuous. Some of the most common types of regression models include Number 1. Linear Regression, which is simply finding a line that fits the data. Its extensions include multiple linear regression, that is, finding a plane of best fit, and polynomial regression, that is, finding a curve for best fit. Next one, Decision Tree. It looks something like this, where each square above is called a node, and the more nodes you have, the more accurate your decision tree will be in general. Next and the third type, Random Forest. These are assembled learning techniques that build off over decision trees and involve creating multiple decision trees using bootstrapped datasets of original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all the predictions of each decision tree, and by relying on the majority wins model, it reduces the risk of error from the individual tree. Next, Neural Network. It is quite popular and is a multi-layered model inspired by the human mind. Like the neurons in our brain, the circle represents a node, the blue circle represents an input layer, the black circle represents a hidden layer, and the green circle represents the output layer. Each node in the hidden layer represents a function that input goes through, ultimately leading to the output in the green circles. Next, Classification. So, with regression types being over, now let's jump to classification. So, in classification, the output is discrete. Some of the most common types of classification models include, first, logistic regression, which is similar to linear regression but is used to model the probability of a finite number of outcomes, typically 2. Next, Support Vector Machine. It is a supervised classification technique that carries an objective to find a hyperlane in n-dimensional space that can distinctly classify the data points. Next, Naive Bayes. It's a classifier which acts as a probabilistic machine learning model used for classification tasks. The crux of the classifier is based on the Bayes theorem. Coming up next, Decision Trees, Random Forests, and Neural Networks. These models follow the same logic as previously explained. The only difference here is that the output is discrete rather than continuous. Now next, let's jump over to Unsupervised Learning. Unlike supervised learning, unsupervised learning is used to draw inferences and find patterns from input data without references to the labeled outcome. Two main methods used in supervised learning include clustering and dimensionality reduction. Clustering involves grouping of data points. It's frequently used for customer segmentation, fraud detection, and document classification. Common clustering techniques include k-means clustering, hierarchical clustering, mean-shift clustering, and density-based clustering. While each technique has different methods in finding clusters, they all aim to achieve the same thing. Coming up next, Dimensionality Reduction. It is a process of reducing dimensions of your feature set or to state simply, reducing the number of features. Most dimensionality reduction techniques can be categorized as either feature elimination or feature extraction. A popular method of dimensionality reduction is called Principal Component Analysis or PCA. Obviously, there's a ton of complexity if we dive into any particular model. To help you with each, I'll be publishing new videos so be sure to smash that subscribe button to be notified on every upload. Next, if this video helped you, be sure to like it and share it with someone who might need it.